<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>JAX, ovvero NumPy sotto steroidi | Italian Association for Machine Learning</title>
    <meta content="GravCMS"  />
<meta content="The Italian Association for Machine Learning (IAML) is a not-for-profit organization with the purpose of promoting knowledge of machine learning in all aspects of the Italian public life, from universities to enterprises and IT professionals."  />
<meta property="og:title" content="JAX, ovvero NumPy sotto steroidi | IAML.it"  />
<meta property="og:image" content="https://iaml.it/blog/jax-intro/lifecycle.png"  />
<meta property="og:url" content="https://iaml.it/blog/jax-intro/"  />
<meta property="og:description" content="JAX, nuovissimo progetto di ricerca targato Google, ha diverse caratteristiche interessanti: oltre a presentarsi come un vero e proprio wrapper di NumPy, fa dell&#039;efficienza uno dei suoi punti di forza, grazie all&#039;uso trasparente di XLA. Infine, ed è forse la novità più intrigante, è una delle prime librerie a puntare fortemente su una anima di pura programmazione funzionale."  />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="icon" type="image/png" href="../user/themes/deliver/images/favicon.png" />

	<!-- Global site tag (gtag.js) - Google Ads: 774709547 --> <script async src="https://www.googletagmanager.com/gtag/js?id=AW-774709547"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'AW-774709547'); </script> 
	
		
                            		                                                <link href="../user/themes/deliver/css-compiled/nucleus.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css-compiled/template.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css/custom.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css/toc.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css/font-awesome.min.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/facebook/css/facebook.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/facebook/assets/unitegallery/css/unite-gallery.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/facebook/assets/unitegallery/themes/default/ug-theme-default.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/markdown-notices/assets/notices.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/breadcrumbs/css/breadcrumbs.css" type="text/css" rel="stylesheet" />
<link href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/events/assets/events.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/form/assets/form-styles.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/mathjax/assets/css/mathjax.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/simplesearch/css/simplesearch.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/highlight/css/zenburn.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/login/css/login.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css/slidebars.min.css" type="text/css" rel="stylesheet" />
<link href="../user/themes/deliver/css/slideme.css" type="text/css" rel="stylesheet" />
<link href="../user/plugins/socialbuttons/vendor/rrssb/css/rrssb.css" type="text/css" rel="stylesheet" />


                                                            <script src="../system/assets/jquery/jquery-2.x.min.js" type="text/javascript" ></script>
<script src="../user/themes/deliver/js/modernizr.custom.71422.js" type="text/javascript" ></script>
<script src="../user/plugins/facebook/assets/unitegallery/js/unitegallery.min.js" type="text/javascript" ></script>
<script src="../user/plugins/facebook/assets/unitegallery/themes/default/ug-theme-default.js" type="text/javascript" ></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js" type="text/javascript" ></script>
<script src="../user/plugins/events/assets/events.js" type="text/javascript" ></script>
<script src="../user/plugins/mathjax/assets/js/mathjax.js" type="text/javascript" ></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" ></script>
<script src="../user/plugins/highlight/js/highlight.pack.js" type="text/javascript" ></script>
<script src="../user/themes/deliver/js/deliver.js" type="text/javascript" ></script>
<script src="../user/themes/deliver/js/slidebars.min.js" type="text/javascript" ></script>
<script src="../user/themes/deliver/js/jquery.slideme2.js" type="text/javascript" ></script>
<script src="../user/plugins/socialbuttons/vendor/rrssb/js/rrssb.min.js" type="text/javascript" ></script>

<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
"palette": {
    "popup": {
        "background": "#4d4d4d",
        "text": "#fff"
    },
    "button": {
        "background": "#f1d600",
        "text": "#000",
        "border": "#f1d600"
    }
},
"position": "bottom",
"theme": "block",
"content": {
    "message": "This website uses cookies to ensure you get the best experience on our website.",
    "dismiss": "Got it!",
    "link": "Learn more",
    "href": "https://cookiesandyou.com"
}
})});
hljs.initHighlightingOnLoad();

</script>


</head>
<body id="top" class="header-lite fullwidth blogstyling">
    <div id="sb-site">
                <header id="header">
                <div class="logo">
                    <h3><a href="../index.html"><img src="../user/pages/images/IAML_logo_viola.png" /></a></h3>
                                            <ul class="social-icons">
            <li>
            <a href="https://twitter.com/iaml_it">
                <i class="fa fa-twitter"></i>            </a>
        </li>
            <li>
            <a href="https://www.linkedin.com/company/iaml/">
                <i class="fa fa-linkedin"></i>            </a>
        </li>
            <li>
            <a href="https://www.facebook.com/machinelearningitalia/">
                <i class="fa fa-facebook"></i>            </a>
        </li>
            <li>
            <a href="blog.rss">
                <i class="fa fa-rss"></i>            </a>
        </li>
    </ul>  
                                    </div>
                <div id="navbar">
                                                            
<ul class="navigation">
                                                        <li class="">
                    <a href="../index.html">
                                                Home
                    </a>
                </li>
                                                                <li class="">
                    <a href="../activities">
                                                Activities
                    </a>
                </li>
                                                                <li class="">
                    <a href="../supporters.html">
                                                Supporters
                    </a>
                </li>
                                                                <li class="">
                    <a href="../member.html">
                                                Become a member
                    </a>
                </li>
                                                                <li class="active">
                    <a href="../blog">
                                                Blog
                    </a>
                </li>
                                                                <li class="">
                    <a href="../governance.html">
                                                Governance
                    </a>
                </li>
                                                                                                                                </ul>                                                   <form class="search-box">
    <input type="search" placeholder="Search..." value="" data-search-input="/search/query" />
    <script>
    jQuery(document).ready(function($){
        var input = $('[data-search-input]');

        input.on('keypress', function(event) {
            if (event.which == 13 && input.val().length > 3) {
                event.preventDefault();
                window.location.href = input.data('search-input') + ':' + input.val();
            }
        });
    });
    </script>
    <i class="fa fa-search"></i>
</form>                    <span class="panel-activation sb-toggle-left navbar-left menu-btn fa fa-bars"></span>
                </div>
        </header>
        
        
                <section id="body" class="">
                            
				<div class="flush-top blog-header blog-header-image" style="background: #B4B093 url(/user/pages/05.blog/blue_header.jpg) no-repeat right;">
            <h1>JAX, ovvero NumPy sotto steroidi</h1>
        </div>
            
        
<div id="breadcrumbs" itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
                                            <a href="../index.html" itemprop="url"><span itemprop="title">Home</span></a>
                        <i class="fa fa-angle-right"></i>
                                                <a href="../blog" itemprop="url"><span itemprop="title">Blog</span></a>
                        <i class="fa fa-angle-right"></i>
                                                <span itemprop="title">JAX, ovvero NumPy sotto steroidi</span>
                        </div>
		
		<div class="blog-content-item g-grid pure-g-r">
			<div id="item" class="g-block pure-u-2-3">
			    <div class="list-item">

    <div class="list-blog-header">
                    <img src="../images/8/9/1/1/d/8911df89521b7f7538f38c7e5460963762c160b8-jaxlogo250px.png" />
        
                    <h4><a href="jax-intro.html">JAX, ovvero NumPy sotto steroidi</a></h4>
        
        <span class="list-blog-date">
            <i class="fa fa-calendar"></i>
            24, Dec
        </span>
                <span class="list-blog-author">
            <i class="fa fa-user"></i>
            Simone Scardapane
        </span>
                       <ul class="tags">
            <i class="fa fa-tag"></i>
                        <li><a href="tagjax.html">jax</a></li>
                        <li><a href="tagjit.html">jit</a></li>
                        <li><a href="taggoogle.html">google</a></li>
                        <li><a href="tagnumpy.html">numpy</a></li>
                        <li><a href="tagvmap.html">vmap</a></li>
                        <li><a href="tagfunzionale.html">funzionale</a></li>
                    </ul>
        
    </div>

	<div>
	<br />
	<!-- AddToAny BEGIN -->
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
<a class="a2a_button_facebook"></a>
<a class="a2a_button_twitter"></a>
<a class="a2a_button_linkedin"></a>
<a class="a2a_button_google_plus"></a>
<a class="a2a_button_email"></a>
</div>
<script async src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->
</div>
	
    <div class="list-blog-padding">

            <p><p>Nell'era dei 'giganti' (TensorFlow, PyTorch, ...), introdurre e studiare una nuova libreria di machine learning potrebbe sembrare controproducente. Eppure <a href="https://github.com/google/jax">JAX</a>, un nuovissimo progetto di ricerca targato Google, ha diverse caratteristiche che lo rendono di interesse ad un vasto pubblico. In primo luogo, si presenta come un vero e proprio <strong>wrapper di NumPy</strong>, rendendo la transizione da quest'ultima libreria quasi immediata. In secondo luogo, fa dell'efficienza uno dei suoi punti di forza, grazie all'uso trasparente di <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md">XLA</a>, un acceleratore di algebra lineare originariamente sviluppato per TensorFlow. Infine, ed è forse la novità più intrigante, è una delle prime librerie a puntare fortemente su una anima di <strong>pura programmazione funzionale</strong>.</p>
<p></p>
<h2>JAX - A confronto con i giganti</h2>
<p>Fondamentalmente, ogni libreria di reti neurali in commercio si può categorizzare sulle base di quattro elementi chiave:</p>
<ol>
<li>Il modo in cui permette di esprimere e manipolare operazioni su tensori (es., <em>grafi computazionali</em> vs. esecuzione <em>eager</em>);</li>
<li>Strumenti e modalità di differenziazione automatica (es., possibilità di avere gradienti di ordine superiore);</li>
<li>Le tecniche che mette a disposizione per accelerare il codice su GPU, sistemi distribuiti, o altro;</li>
<li>I moduli ad alto livello eventualmente presenti per costruire ed allenare le reti neurali stesse.</li>
</ol>
<p>Questi quattro punti permettono una enorme differenziazione nell'ecosistema: <a href="https://keras.io">Keras</a>, ad esempio, si è originariamente affermata puntando quasi tutto sul punto (4), lasciando ad un backend a scelta gli altri compiti. <a href="https://github.com/HIPS/autograd">Autograd</a>, invece, nel 2015 puntò sui primi due punti, permettendo di scrivere codice usando solo costrutti "classici" di Python e NumPy, fornendo poi tantissime opzioni per il secondo punto. La semplicità di Autograd influenzò moltissimo lo sviluppo delle librerie a seguire, ma essa fu penalizzata dalla netta carenza dei punti (3) e (4), ovvero tecniche adeguate per accelerare il codice e sviluppare ad alto livello.</p>
<p>JAX è, fondamentalmente, <strong>la versione 2.0 di Autograd</strong>: ne riprende l'intera filosofia, aumentandola con diverse tecniche di accelerazione su GPU/TPU e con piccole librerie ad alto livello per la prototipazione di modelli e la loro ottimizzazione. Può quindi essere di enorme interesse per diverse fasce di utenti: per chi volesse solo accelerare codice NumPy, o per chi ama sviluppare "dal basso" senza spostarsi troppo dalla familiarità di NumPy. Ed infine, punto non da poco, per chi cercasse una libreria dall'animo puramente funzionale.</p>
<p>Nel resto di questo articolo vediamo rapidamente le funzionalità principali di JAX disponibili al momento (versione 0.12), seguendo parti del tutorial originario ed estendendole quando necessario.</p>
<div class="notices blue">
<p>Tutto il codice di questo articolo è disponibile <a href="https://colab.research.google.com/drive/1CeGy6qIy65eGAi_Co-PHyeW3REtEpKMQ">su un notebook Google Colab</a>.</p>
</div>
<h2>Installare JAX</h2>
<p>Installare JAX richiede di compilare XLA sulla propria architettura, seguendo <a href="https://github.com/google/jax#installation">le istruzioni sul sito</a>. Su Google Colab trovare una versione binaria già pronta che potete installare molto facilmente:</p>
<blockquote>
<p>!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl</p>
<p>!pip install --upgrade jax</p>
</blockquote>
<p>Il resto del tutorial presuppone questo setup.</p>
<h2>JAX core 1: il wrapper NumPy</h2>
<p>Cominciamo dalle basi: un po' di istruzioni a caso di NumPy.</p>
<pre><code class="language-py">import numpy as np

x = np.ones((5000, 5000))
y = np.arange(5000)

z = np.sin(x) + np.cos(y)</code></pre>
<p>Il codice equivalente di JAX richiede solo di importare il wrapper apposito per NumPy:</p>
<pre><code class="language-py">import jax.numpy as np # Unica differenza!

x = np.ones((5000, 5000))
y = np.arange(5000)

z = np.sin(x) + np.cos(y) </code></pre>
<p>Già così l'utilizzo di XLA ci garantisce una buona accelerazione: sul backend GPU di Colab, il codice di prima gira in 30 ms contro circa 480 ms del codice NumPy. Non tutte le funzioni di NumPy/SciPy sono ancora implementate, ma dovrebbero esserlo a regime della libreria. Se oltre ad accelerare codice volete usare anche tutte le funzionalità che seguono (es., auto-differenziazione), allora ci sono alcuni vincoli aggiuntivi sul codice che potete scrivere, derivanti anche dalla natura funzionale della libreria: ad esempio, <a href="https://github.com/google/jax#whats-supported">non potete modificare i valori di un array tramite indicizzazione</a>.</p>
<h2>JAX core 2: il compilatore JIT</h2>
<p>Il codice di prima accelera ogni istruzione tramite XLA, ma in generale potreste voler accelerare <em>interi blocchi di codice</em> sfruttando eventuali parallelismi al loro interno. In questo caso, JAX mette a disposizione un meccanismo di compilazione tramite <em>tracing</em> molto simile al <a href="https://iaml.it/blog/alle-prese-con-pytorch-parte-5">compilatore JIT di PyTorch</a>.</p>
<p>Il funzionamento è molto semplice, possiamo usare un'annotazione (od una funzione esplicita) per indicare a JAX cosa compilare:</p>
<pre><code class="language-py">from jax import jit

@jit
def fn(x, y):
  z = np.sin(x)
  w = np.cos(y)
  return z + w

# Alternativa senza annotazione:
# fn = jit(fn)</code></pre>
<p>In questo caso JAX compilerà la funzione al suo primo utilizzo, riusando la versione ottimizzata successivamente. Sul backend GPU di Colab otteniamo un ulteriore guadagno del 30% circa, riducendo il tempo medio di esecuzione a 20 ms. L'utilizzo del compilatore richiede però qualche vincolo aggiuntivo, <a href="https://github.com/google/jax#whats-supported">in particolare sull'indicizzamento e le istruzioni condizionali e di flusso</a>. </p>
<h2>JAX medium 1: auto-differenziazione</h2>
<p>Il meccanismo di auto-differenziazione è simile a quello presente in altre librerie: data una funzione Python con una serie di manipolazioni su tensori, possiamo automaticamente ottenerne una seconda che ne calcola il gradiente in maniera automatica:</p>
<pre><code class="language-py">from jax import grad

@jit
def simple_fun(x):
  return np.sin(x) / x

# Ritorna il gradiente di simple_fun rispetto ad x  
grad_simple_fun = grad(simple_fun)</code></pre>
<p>Possiamo concatenare più chiamate per ottenere gradienti di ordine superiore:</p>
<pre><code class="language-py"># Calcola la seconda derivata (diagonale dell'Hessiana)
grad_grad_simple_fun = grad(grad(simple_fun))</code></pre>
<p>Possiamo graficare il tutto!</p>
<pre><code class="language-py">import matplotlib.pyplot as plt
plt.plot(x_range, simple_fun(x_range), 'b')
plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], '--r')
plt.plot(x_range, [grad_grad_simple_fun(xi) for xi in x_range], '--g')
plt.show()</code></pre>
<figure role="group">
        <img src="https://iaml.it/blog/jax-intro/images/simple_fun.png" />
        </figure>
<h2>JAX medium 2: vettorizzazione avanzata con vmap</h2>
<p>Oltre ad accelerare ogni singola istruzione, ed al tracer, JAX mette a disposizione un terzo meccanismo di accelerazione, da usare quando vogliamo applicare la stessa funzione su uno o più assi di un tensore. Vediamo un esempio pratico riprendendo il calcolo del gradiente di prima:</p>
<pre><code class="language-py"># Calcolo del gradiente (naive)
[grad_simple_fun(xi) for xi in x_range]</code></pre>
<p>Come in molte librerie, JAX suppone che la funzione che state differenziando abbia un solo output. Per calcolare più gradienti in parallelo, in questa situazione abbiamo dovuto richiamarla separatamente per ogni valore. Possiamo ottenere lo stesso effetto con una chiamata all'operatore <code>vmap</code>:</p>
<pre><code class="language-py">from jax import vmap
grad_vect_simple_fun = vmap(grad_simple_fun)(x_range)</code></pre>
<p><code>vmap</code> restituisce una nuova funzione che applica la funzione originaria (<code>grad_simple_fun</code>) su un intero vettore. In questo semplice modo, otteniamo uno speedup di 100x sull'esecuzione (4 ms contro 400 ms)!</p>
<p>In generale, <code>grad</code>, <code>jit</code> e <code>vmap</code> sono tre esempi di quelle che JAX chiama <strong>trasformazioni componibili</strong>, ovvero operatori applicabili ad una generica funzione, e componibili fra loro.</p>
<figure role="group">
        <img src="https://iaml.it/blog/jax-intro/images/lifecycle.png"alt="JAX lifecycle" />
        </figure>
<figcaption>Schematizzazione del "ciclo di vita" di una funzione in JAX. Fonte: <a href="https://github.com/google/jax#how-it-works">JAX GitHub</a>.</figcaption>
<h2>JAX core 2.5: generazione di numeri pseudocasuali</h2>
<p>Prima di passare a vedere alcuni costrutti di alto livello per l'allenamento di reti neurali, è necessario discutere brevemente del modo in cui JAX gestisce i numeri pseudocasuali. JAX implementa un proprio PRNG che, a differenza di quello NumPy, ha un'interfaccia puramente funzionale, ovvero senza <em>side effects</em>: tra le altre cose, una chiamata ad un metodo pseudocasuale (es., <code>randn</code>) non può modificare lo stato interno del generatore.</p>
<p>Per questo, gli utenti in JAX devono esplicitamente richiamare e manipolare lo stato del PRNG, nella forma di una chiave:</p>
<pre><code class="language-py">from jax import random

# Genera una chiave
key = random.PRNGKey(0)

# La chiave va passata esplicitamente durante la creazione di un array di numeri pseudocasuali
print(random.normal(key, shape=(3,)))</code></pre>
<p>Differenza essenziale, come detto, è che la chiave non viene modificata dalla chiamata a <code>random.normal</code>: chiamate successive alla funzione con la stessa chiave produrrebbero array uguali. Per modificare la chiave, dobbiamo 'sdoppiarla' con una chiamata apposita:</p>
<pre><code class="language-py"># Ottiene due chiavi separate
key, new_key = random.split(key)

# Usando due chiavi diverse, abbiamo risultati diversi
print(random.normal(key, shape=(3,)))
print(random.normal(new_key, shape=(3,)))</code></pre>
<p>Questo è forse l'aspetto meno intuitivo e più prono ad errori della libreria allo stato attuale, e potrebbe essere modificato / migliorato in futuro.</p>
<h2>JAX advanced 1: costruire reti neurali con STAX</h2>
<p>JAX contiene al suo interno anche delle mini-librerie che ne mostrano le potenzialità. Una di queste, STAX, è dedicata a costruire reti neurali, con un'interfaccia simile ad altri framework di deep learning. Ad esempio, possiamo costruire una rete come "stack" di diversi "strati":</p>
<pre><code class="language-py">from jax.experimental import stax
from jax.experimental.stax import Dense, Relu, LogSoftmax

net_init, net_apply = stax.serial(
    Dense(10), Relu,
    Dense(3), LogSoftmax,
)</code></pre>
<p>A differenza di altri framework, però, la rete neurale così ottenuta rispetta un'interfaccia funzionale e non ad oggetti: in particolare, la rete è definita da una coppia di funzioni, rispettivamente per l'inizializzazione dei parametri e la predizione.</p>
<pre><code class="language-py"># Inizializza la rete con quattro input
out_shape, net_params = net_init((-1, 4))

# Ottiene le predizioni della rete
print(net_apply(net_params, Xtrain))</code></pre>
<h2>JAX advanced 2: ottimizzazione con minmax</h2>
<p>La seconda libreria di JAX, minmax, permette invece di ottimizzare funzioni costo. Supponiamo di definire una funzione di costo per la nostra rete (es., cross-entropia):</p>
<pre><code class="language-py">def loss(params):
  predictions = net_apply(params, Xtrain)
  return - np.mean(ytrain * predictions)</code></pre>
<p>All'interno di minmax troviamo diversi algoritmi già implementati, tra cui Adam:</p>
<pre><code class="language-py">from jax.experimental import minmax
opt_init, opt_update = minmax.adam(step_size=0.01)</code></pre>
<p>Anche l'ottimizzatore, come la rete neurale, non è ad oggetti ma è definito da due funzioni, una per l'inizializzazione ed una per il passo di aggiornamento (dati i gradienti). Vediamo il codice per un singolo passo di ottimizzazione:</p>
<pre><code class="language-py">@jit
def step(i, opt_state):
  # Parametri dell'algoritmo di ottimizzazione
  params = minmax.get_params(opt_state)
  # Gradienti della funzione costo
  g = grad(loss)(params)
  # Passo di aggiornamento
  return opt_update(i, g, opt_state)</code></pre>
<p>Ed il codice complessivo dell'ottimizzazione:</p>
<pre><code class="language-py"># Inizializzazione dell'ottimizzatore
opt_state = opt_init(net_params)
for i in range(100):
  opt_state = step(i, opt_state)
# Parametri finali dopo l'allenamento
net_params = minmax.get_params(opt_state)</code></pre>
<figure role="group">
        <img src="https://iaml.it/blog/jax-intro/images/funzione_costo.png" />
        </figure>
<p>minmax permette di ottimizzare funzioni costo nelle quali i tensori sono definiti all'interno di liste o dizionari in maniera completamente trasparente: l'importante è che i gradienti passati alla funzione di update rispettino lo stesso formato.</p>
<h2>Ricapitolando</h2>
<p>JAX è una libreria molto giovane ma molto promettente per un vasto pubblico, sia che decida di implementare dal basso sfruttando la forte accelerazione della libreria, sia che si senta ispirato dalla sua interfaccia fondamentalmente funzionale. In attesa del rilascio della prima versione stabile, speriamo che questo breve tutorial vi abbia stuzzicato l'interesse!</p>
<hr />
<p>Se questo articolo ti è piaciuto e vuoi tenerti aggiornato sulle nostre attività, ricordati che l'<a href="../member.html">iscrizione all'Italian Association for Machine Learning</a> è gratuita! Puoi seguirc su <a href="https://www.facebook.com/machinelearningitalia/">Facebook</a>, <a href="https://www.linkedin.com/company/iaml/">LinkedIn</a>, e <a href="https://twitter.com/iaml_it">Twitter</a>.</p></p>
            
    
        <p class="prev-next">
                            <a class="button" href="alle-prese-con-pytorch-parte-5.html"><i class="fa fa-chevron-left"></i> Previous Post</a>
            
                            <a class="button" href="panoramica-storia-recente-nlp.html">Next Post <i class="fa fa-chevron-right"></i></a>
                    </p>
    
    </div>
</div>
			</div>
			<div id="sidebar" class="g-block size-1-3 pure-u-1-3">
				<div class="sidebar-content">
    <h4>Search the blog</h4>
    <input type="text" placeholder="Search..." value="" data-searchsidebar-input="/search/query" />
<script>
jQuery(document).ready(function($){
    var input = $('[data-searchsidebar-input]');

    input.on('keypress', function(event) {
        if (event.which == 13 && input.val().length > 3) {
            event.preventDefault();
            window.location.href = input.data('searchsidebar-input') + ':' + input.val();
        }
    });
});
</script>
</div>
<!--
<div class="sidebar-content">
	<h4>Some Text Widget</h4>
	<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna.</p>
</div>
!-->
<div class="sidebar-content">
    <h4>Categories</h4>
    

<ul class="archives">
        <li>
        <a href="categoryTutorials.html">Tutorials </a> (16)
    </li>
        <li>
        <a href="categoryDiscussions.html">Discussions </a> (12)
    </li>
        <li>
        <a href="categoryAnnouncements.html">Announcements </a> (4)
    </li>
        <li>
        <a href="categoryTutorials (English).html">Tutorials (English) </a> (4)
    </li>
        <li>
        <a href="categoryArticles' summaries.html">Articles' summaries </a> (3)
    </li>
        <li>
        <a href="categoryDiscussions (English).html">Discussions (English) </a> (2)
    </li>
        <li>
        <a href="categoryFocus-on.html">Focus-on </a> (1)
    </li>
        <li>
        <a href="categoryReviews.html">Reviews </a> (1)
    </li>
        <li>
        <a href="categoryDiscussion.html">Discussion </a> (1)
    </li>
    </ul>
</div>
<div class="sidebar-content">
    <h4>Archives</h4>
	<ul class="archives">
    <li>
    	<a href="archives_monthapr_2020.html">
        <span class="archive_date">April 2020</span>
                <span>(1)</span>
                </a>
    </li>
</ul>
</div>
<div class="sidebar-content">
    <h4>Popular Tags</h4>
    

<ul class="archives">
        <li>
        <a href="tagdeep learning.html">deep learning </a> (11)
    </li>
        <li>
        <a href="tagpytorch.html">pytorch </a> (9)
    </li>
        <li>
        <a href="tagreti neurali.html">reti neurali </a> (5)
    </li>
        <li>
        <a href="taggoogle.html">google </a> (4)
    </li>
        <li>
        <a href="tagjit.html">jit </a> (4)
    </li>
        <li>
        <a href="tagtensorflow.html">tensorflow </a> (4)
    </li>
        <li>
        <a href="tagottimizzazione.html">ottimizzazione </a> (4)
    </li>
        <li>
        <a href="tagrete neurale.html">rete neurale </a> (3)
    </li>
        <li>
        <a href="tagtime series.html">time series </a> (3)
    </li>
        <li>
        <a href="tagkeras.html">keras </a> (3)
    </li>
        <li>
        <a href="tagreti convolutive.html">reti convolutive </a> (3)
    </li>
        <li>
        <a href="tagpipeline.html">pipeline </a> (2)
    </li>
        <li>
        <a href="tagsklearn.html">sklearn </a> (2)
    </li>
        <li>
        <a href="tagautodiff.html">autodiff </a> (2)
    </li>
        <li>
        <a href="tagautomatic differentation.html">automatic differentation </a> (2)
    </li>
        <li>
        <a href="tagreverse-mode.html">reverse-mode </a> (2)
    </li>
        <li>
        <a href="tagderivate.html">derivate </a> (2)
    </li>
        <li>
        <a href="tagdifferenziazione.html">differenziazione </a> (2)
    </li>
        <li>
        <a href="tagmodel selection.html">model selection </a> (2)
    </li>
        <li>
        <a href="tagcross validation.html">cross validation </a> (2)
    </li>
        <li>
        <a href="tagc++.html">c++ </a> (2)
    </li>
        <li>
        <a href="tagnumpy.html">numpy </a> (2)
    </li>
        <li>
        <a href="tagvmap.html">vmap </a> (2)
    </li>
        <li>
        <a href="tagcaffe.html">caffe </a> (2)
    </li>
        <li>
        <a href="tagcompiler.html">compiler </a> (2)
    </li>
        <li>
        <a href="tagjax.html">jax </a> (2)
    </li>
        <li>
        <a href="tagcodemotion.html">codemotion </a> (1)
    </li>
        <li>
        <a href="tagbias.html">bias </a> (1)
    </li>
        <li>
        <a href="tagdiscrimination.html">discrimination </a> (1)
    </li>
        <li>
        <a href="tagfairness.html">fairness </a> (1)
    </li>
        <li>
        <a href="tagiaml.html">iaml </a> (1)
    </li>
        <li>
        <a href="tagdatabase.html">database </a> (1)
    </li>
        <li>
        <a href="tagiperparametri.html">iperparametri </a> (1)
    </li>
        <li>
        <a href="tagautograph.html">autograph </a> (1)
    </li>
        <li>
        <a href="taghead.html">head </a> (1)
    </li>
        <li>
        <a href="tagmulti-task.html">multi-task </a> (1)
    </li>
        <li>
        <a href="taglearning.html">learning </a> (1)
    </li>
        <li>
        <a href="tagnovità.html">novità </a> (1)
    </li>
        <li>
        <a href="tagdev summit.html">dev summit </a> (1)
    </li>
        <li>
        <a href="tagcustom estimator.html">custom estimator </a> (1)
    </li>
        <li>
        <a href="taghyperopt.html">hyperopt </a> (1)
    </li>
        <li>
        <a href="taggoodfellow.html">goodfellow </a> (1)
    </li>
        <li>
        <a href="tagnlp.html">nlp </a> (1)
    </li>
        <li>
        <a href="tagdati mancanti.html">dati mancanti </a> (1)
    </li>
        <li>
        <a href="tagtransformer.html">transformer </a> (1)
    </li>
        <li>
        <a href="tagattenzione.html">attenzione </a> (1)
    </li>
        <li>
        <a href="tagrobocop.html">robocop </a> (1)
    </li>
        <li>
        <a href="tagyolo.html">yolo </a> (1)
    </li>
        <li>
        <a href="tagobject detection.html">object detection </a> (1)
    </li>
        <li>
        <a href="tagbayes.html">bayes </a> (1)
    </li>
        <li>
        <a href="tagautoencoders.html">autoencoders </a> (1)
    </li>
        <li>
        <a href="tagvariational.html">variational </a> (1)
    </li>
        <li>
        <a href="tageager.html">eager </a> (1)
    </li>
        <li>
        <a href="tagimputazione.html">imputazione </a> (1)
    </li>
        <li>
        <a href="tagCIFAR.html">CIFAR </a> (1)
    </li>
        <li>
        <a href="tagword embedding.html">word embedding </a> (1)
    </li>
        <li>
        <a href="tagMNIST.html">MNIST </a> (1)
    </li>
        <li>
        <a href="tagimmagini.html">immagini </a> (1)
    </li>
        <li>
        <a href="tagclassificazione.html">classificazione </a> (1)
    </li>
        <li>
        <a href="tagkpi.html">kpi </a> (1)
    </li>
        <li>
        <a href="tagreprogramming.html">reprogramming </a> (1)
    </li>
        <li>
        <a href="tagadversarial.html">adversarial </a> (1)
    </li>
        <li>
        <a href="tagbrowser.html">browser </a> (1)
    </li>
        <li>
        <a href="tagjavascript.html">javascript </a> (1)
    </li>
        <li>
        <a href="tagreti ricorsive.html">reti ricorsive </a> (1)
    </li>
        <li>
        <a href="tagreti ricorrenti.html">reti ricorrenti </a> (1)
    </li>
        <li>
        <a href="tagftth.html">ftth </a> (1)
    </li>
        <li>
        <a href="tagadversarial example.html">adversarial example </a> (1)
    </li>
        <li>
        <a href="tagmanagement.html">management </a> (1)
    </li>
        <li>
        <a href="tagrobotica.html">robotica </a> (1)
    </li>
        <li>
        <a href="tagocr.html">ocr </a> (1)
    </li>
        <li>
        <a href="tagfocus.html">focus </a> (1)
    </li>
        <li>
        <a href="tagiphone.html">iphone </a> (1)
    </li>
        <li>
        <a href="tagpython.html">python </a> (1)
    </li>
        <li>
        <a href="tagface id.html">face id </a> (1)
    </li>
        <li>
        <a href="tagmomento.html">momento </a> (1)
    </li>
        <li>
        <a href="tagadam.html">adam </a> (1)
    </li>
        <li>
        <a href="tagneuroscienza.html">neuroscienza </a> (1)
    </li>
        <li>
        <a href="tagonde cerebrali.html">onde cerebrali </a> (1)
    </li>
        <li>
        <a href="tagtorchvision.html">torchvision </a> (1)
    </li>
        <li>
        <a href="taglatin.html">latin </a> (1)
    </li>
        <li>
        <a href="tagpretrained.html">pretrained </a> (1)
    </li>
        <li>
        <a href="tagrete convolutiva.html">rete convolutiva </a> (1)
    </li>
        <li>
        <a href="tagautograd.html">autograd </a> (1)
    </li>
        <li>
        <a href="tagswish.html">swish </a> (1)
    </li>
        <li>
        <a href="tagattivazione.html">attivazione </a> (1)
    </li>
        <li>
        <a href="tagcheckpoint.html">checkpoint </a> (1)
    </li>
        <li>
        <a href="tagtensori.html">tensori </a> (1)
    </li>
        <li>
        <a href="tagvariabili.html">variabili </a> (1)
    </li>
        <li>
        <a href="taglineare.html">lineare </a> (1)
    </li>
        <li>
        <a href="tagregressione.html">regressione </a> (1)
    </li>
        <li>
        <a href="tagconvolutional networks.html">convolutional networks </a> (1)
    </li>
        <li>
        <a href="tagVatican.html">Vatican </a> (1)
    </li>
        <li>
        <a href="tagproject.html">project </a> (1)
    </li>
        <li>
        <a href="tagkernel.html">kernel </a> (1)
    </li>
        <li>
        <a href="tagICLR.html">ICLR </a> (1)
    </li>
        <li>
        <a href="tagipotesi.html">ipotesi </a> (1)
    </li>
        <li>
        <a href="tagsparsità.html">sparsità </a> (1)
    </li>
        <li>
        <a href="tagfunzionale.html">funzionale </a> (1)
    </li>
        <li>
        <a href="tagfunctional.html">functional </a> (1)
    </li>
        <li>
        <a href="tagadversarial attack.html">adversarial attack </a> (1)
    </li>
        <li>
        <a href="tagkmeans.html">kmeans </a> (1)
    </li>
        <li>
        <a href="taganalysis.html">analysis </a> (1)
    </li>
        <li>
        <a href="tagclustering.html">clustering </a> (1)
    </li>
        <li>
        <a href="tagGoogle.html">Google </a> (1)
    </li>
        <li>
        <a href="tagregression.html">regression </a> (1)
    </li>
        <li>
        <a href="tagJAX.html">JAX </a> (1)
    </li>
        <li>
        <a href="taggaussian process.html">gaussian process </a> (1)
    </li>
        <li>
        <a href="tagensemble.html">ensemble </a> (1)
    </li>
        <li>
        <a href="tagboosting.html">boosting </a> (1)
    </li>
        <li>
        <a href="taggradient.html">gradient </a> (1)
    </li>
        <li>
        <a href="tagsemi-supervised learning.html">semi-supervised learning </a> (1)
    </li>
        <li>
        <a href="tagdocument classification.html">document classification </a> (1)
    </li>
        <li>
        <a href="taggraphs.html">graphs </a> (1)
    </li>
        <li>
        <a href="tagvariables.html">variables </a> (1)
    </li>
        <li>
        <a href="taglinear.html">linear </a> (1)
    </li>
        <li>
        <a href="tagk-NN.html">k-NN </a> (1)
    </li>
    </ul>
</div>
<div class="sidebar-content syndicate">
    <h4>Syndicate</h4>
    <a class="button" href="../blog.atom"><i class="fa fa-rss-square"></i> Atom 1.0</a>
    <a class="button" href="../blog.rss"><i class="fa fa-rss-square"></i> RSS</a>
</div>
			</div>
		</div>
	
                        <div class="modular-row footer ">
    <div class="footer-items">
        <div class="footer-module large">
		<h4>About</h4>
                            <p>Italian Association for Machine Learning (C.F. 97949550582)</p>
            			<p>Write us: info@iaml.it</p>
        </div>
        <div class="footer-module"><h4>Address</h4>
            <p>
                                    <span><strong>Operational office</strong></span>
                                    <span>IAML c/o Pi Campus, via Indonesia 23, 00144 Rome</span>
                                    <span><strong>Legal office</strong></span>
                                    <span>Via Cassia 964, 00189, Rome</span>
                            </p>
        </div>
        <div class="footer-module"><h4>Quick Links</h4>
         <ul class="quickmenu">
                            <li><i class="fa fa-chevron-right"></i><a href="../home">Home</a></li>
                            <li><i class="fa fa-chevron-right"></i><a href="https://iaml.it/documents">Documents (Italian)</a></li>
                    </ul>
    </div>
   
</div>
<hr>
<div class="footer-modules">
    <div class="footer-copyright">
        Copyright 2018 IAML.IT. All Rights Reserved.
    </div>
    <div class="footer-menu">
    <ul class="othermenu">
           <li><a href="https://learn.getgrav.org/">Powered by Grav</a></li>
           <li><a href="https://github.com/getgrav/grav-theme-deliver">Theme (adapted) from Deliver</a></li>
        </ul>
    </div>
</div>
</div>                    </section>
        
    </div>
    <div class="sb-slidebar sb-left sb-width-thin">
        <div id="panel">
        
<ul class="navigation">
                                                        <li class="">
                    <a href="../index.html">
                                                Home
                    </a>
                </li>
                                                                <li class="">
                    <a href="../activities">
                                                Activities
                    </a>
                </li>
                                                                <li class="">
                    <a href="../supporters.html">
                                                Supporters
                    </a>
                </li>
                                                                <li class="">
                    <a href="../member.html">
                                                Become a member
                    </a>
                </li>
                                                                <li class="active">
                    <a href="../blog">
                                                Blog
                    </a>
                </li>
                                                                <li class="">
                    <a href="../governance.html">
                                                Governance
                    </a>
                </li>
                                                                                                                                </ul>                   </div>
    </div>
        <script src="../user/plugins/simplesearch/js/simplesearch.js" type="text/javascript" ></script>

    <script>
    $(function () {
        $(document).ready(function() {
          $.slidebars({
            hideControlClasses: true,
            scrollLock: true
          });
        });
    });
    </script>
    </body>
</html>


